---
title: "Homework 2"
author: "Zhezheng Jin"
output:
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
editor_options: 
  chunk_output_type: console
--- 

\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
```

```{r, echo = TRUE, message = FALSE, results='hide', warning=FALSE}
library(tidyverse)
library(caret)
library(tidymodels)
```

```{r}
# Data Import
data = read_csv("College.csv") %>%
janitor::clean_names() %>%
select(-college) %>%
relocate(outstate)

# data partition
set.seed(80)
indexTrain <- createDataPartition(y = data$outstate, p = 0.8, list = FALSE)
trainData <- data[indexTrain, ]
testData <- data[-indexTrain, ]
```

The "College" dataset contains `r ncol(data)` columns and `r nrow(data)` observations after omitting the `college` variable. Then we partition the dataset into two parts: training data (80%) and test data (20%), where the training data and test data contains `r nrow(trainData)` and `r nrow(testData)` rows, respectively.

## (a)  smoothing spline models 

```{r}
perc_alumni.grid <- seq(from =1, to =65,by =1)

# Fit smoothing spline models using perc.alumni as the only predictor of Outstate for
# a range of degrees of freedom
# write a function to apply multiple df's to smoothing spline
ss.data.func <- function(trainData, perc_alumni.grid, df_seq) {
  ss.list <- lapply(df_seq, function(df) {
    fit.ss <- smooth.spline(trainData$perc_alumni, trainData$outstate, df = df)
    pred.ss <- predict(fit.ss, x = perc_alumni.grid)
    pred.ss.df <- data.frame(pred = pred.ss$y,
                              perc.alumni = perc_alumni.grid, df = df)
    return(pred.ss.df)
  })
  ss.data <- do.call(rbind, ss.list)
  return(ss.data)
}

p <- ggplot(data = trainData, aes(x = perc_alumni, y = outstate)) +
  geom_point(color = rgb(.2, .4, .2, .5)) +
  geom_line(aes(x = perc.alumni, y = pred, group = df, color = df),
            data = ss.data.func(trainData, perc_alumni.grid, 1:16))

# degree of freedom obtained by generalized cross-validation
fit.ss <- smooth.spline(trainData$perc_alumni, trainData$outstate)
# degree of freedom obtained by generalized
# cross-validation
fit.ss$df

pred.ss <- predict(fit.ss, x = perc_alumni.grid)
pred.ss.df <- data.frame(pred = pred.ss$y,
                         perc_alumni = perc_alumni.grid)

# plot the resulting fits
p + geom_line(aes(x = perc_alumni, y = pred), data = pred.ss.df,
              color = rgb(.8, .1, .1, 1)) + theme_bw()
```



## (b)  MARS

```{r}
mars_grid <- expand.grid(degree = 1:3,
                         nprune = 2:25)

set.seed(80)
mars.fit <- train(x, trainData$outstate,
                  method = "earth",
                  tuneGrid = mars_grid,
                  trControl = ctrl1)

ggplot(mars.fit)

mars.fit$bestTune

coef(mars.fit$finalModel)

# Present the partial dependence plot of an arbitrary predictor
p1 <- pdp::partial(mars.fit, pred.var = c("perc_alumni"), grid.resolution = 10) %>% autoplot()

p2 <- pdp::partial(mars.fit, pred.var = c("perc_alumni", "apps"), 
                   grid.resolution = 10) %>%
      pdp::plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, 
                       screen = list(z = 20, x = -60))

gridExtra::grid.arrange(p1, p2, ncol = 2)

# test error
mars.pred <- predict(mars.fit, newdata = x2)
mars.test.error <- mean((mars.pred - testData$outstate)^2)
mars.test.error
```



## (c)  GAM

```{r}
# matrix of predictors 
x <- model.matrix(outstate~.,trainData)[,-1]
x2 <- model.matrix(outstate~.,testData)[,-1]

ctrl1 <- trainControl(method = "cv", number = 10)

# Fit a generalized additive model (GAM) using all the predictors
set.seed(80)
gam.fit <- train(x, trainData$outstate,
                 method = "gam",
                 tuneGrid = data.frame(method = "GCV.Cp", select = TRUE),
                 trControl = ctrl1)

gam.fit$bestTune

gam.fit$finalModel

# Plot the results
par(mar = c(1, 1, 1, 1), mfrow=c(4,4))
plot(gam.fit$finalModel)

# test error
gam.pred <- predict(gam.fit, newdata = x2)
gam.test.error <- mean((gam.pred - testData$outstate)^2)
gam.test.error
```



## (d) MARS model & linear model comparison

```{r}
# Fit the linear model
set.seed(80)
lm.fit <- train(x, trainData$outstate, method = "lm", trControl = ctrl1)

# Test error
lm.pred <- predict(lm.fit, newdata = x2)
lm.test.error <- mean((lm.pred - testData$outstate)^2)
lm.test.error

# resamples
resamp <- resamples(list(lm = lm.fit,
                         mars = mars.fit,
                         gam = gam.fit))
summary(resamp)

# RMSE box-plot between models
bwplot(resamp, metric = "RMSE")
```

Since MARS model has a obvious lower mean RMSE, we prefer the use of MARS model over a linear model when predicting the out-of-state tuition in this data example. 

Usually, MARS can outperform linear models, especially when the relationships between predictor variables and the response variable are non-linear. MARS does this by creating piecewise linear models, which can be more flexible than a linear model.

However, whether MARS is a better approach compared to a linear model depends on the specific application and the nature of the data. In some cases, the relationships between predictor variables and the response variable may be linear, in which case a linear model would be more appropriate.

In summary, the choice of modeling technique should be based on the nature of the data and the specific research question being addressed. It may also be useful to compare the performance of different modeling techniques using cross-validation to determine which approach is best for a particular application.



