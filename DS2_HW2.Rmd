---
title: "Homework 2"
author: "Zhezheng Jin"
output:
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
editor_options: 
  chunk_output_type: console
--- 

\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
```

```{r, echo = TRUE, message = FALSE, results='hide', warning=FALSE}
library(tidyverse)
library(caret)
library(tidymodels)
library(earth)
library(splines)
library(mgcv)
library(pdp)
library(bayesQR)
```

```{r}
# Data Import
data = read_csv("College.csv") %>%
janitor::clean_names() %>%
select(-college) %>%
relocate(outstate)

# data partition
set.seed(80)
indexTrain <- createDataPartition(y = data$outstate, p = 0.8, list = FALSE)
trainData <- data[indexTrain, ]
testData <- data[-indexTrain, ]
```

The "College" dataset contains `r ncol(data)` columns and `r nrow(data)` observations after omitting the `college` variable. Then we partition the dataset into two parts: training data (80%) and test data (20%), where the training data and test data contains `r nrow(trainData)` and `r nrow(testData)` rows, respectively.

```{r}
# matrix of predictors 
x <- model.matrix(outstate~.,trainData)[,-1]
x2 <- model.matrix(outstate~.,testData)[,-1]

# vector of response
y <- trainData$outstate
y2 <- testData$outstate

# 10-fold cv on best
ctrl1 <- trainControl(method = "cv", number = 10)
```


## (a)  smoothing spline models 

```{r}
# the range of perc_alumni is [2, 64]
perc_alumni.grid <- seq(from =2, to =64,by =1)

# write a function to apply multiple df's to smoothing spline
ss.data.func <- function(trainData, perc_alumni.grid, df_seq) {
  ss.list <- lapply(df_seq, function(df) {
    fit.ss <- smooth.spline(trainData$perc_alumni, trainData$outstate, df = df)
    pred.ss <- predict(fit.ss, x = perc_alumni.grid)
    pred.ss.df <- data.frame(pred = pred.ss$y,
                              perc.alumni = perc_alumni.grid, df = df)
    return(pred.ss.df)
  })
  ss.data <- do.call(rbind, ss.list)
  return(ss.data)
}

p <- ggplot(data = trainData, aes(x = perc_alumni, y = outstate)) +
  geom_point(color = rgb(.2, .4, .2, .5)) +
  geom_line(aes(x = perc.alumni, y = pred, group = df, color = df),
            data = ss.data.func(trainData, perc_alumni.grid, 1:16))

# degree of freedom obtained by generalized cross-validation
fit.ss <- smooth.spline(trainData$perc_alumni, trainData$outstate)
fit.ss$df

pred.ss <- predict(fit.ss, x = perc_alumni.grid)
pred.ss.df <- data.frame(pred = pred.ss$y,
                         perc_alumni = perc_alumni.grid)

# plot the resulting fits
p + geom_line(aes(x = perc_alumni, y = pred), 
              data = pred.ss.df,color = rgb(.8, .1, .1, 1)) + 
  theme_bw()
```

We can see from the plot that the curve is asymptotically to a line with degree of freedom decreasing.  The appropriate degree of freedom for the model I choose is the df obtained by generalized cross-validation: `r fit.ss$df`, and we can see the resulting fits curve in red is pretty smooth, which means the model is less sensitive to noise in the data. The out-of-state tuition `outstate` increases by the percentage of alumni who donate `perc.alumni`. Therefore, the criteria for determining the best choice of degree of freedom would include minimization of the generalized cross-validation score. 

## (b)  MARS

```{r}
mars_grid <- expand.grid(degree = 1:3,
                         nprune = 2:25)

set.seed(80)
mars.fit <- train(x, y,
                  method = "earth",
                  tuneGrid = mars_grid,
                  trControl = ctrl1)

ggplot(mars.fit)

mars.fit$bestTune

coef(mars.fit$finalModel)

# Present the partial dependence plot of an arbitrary predictor
p1 <- pdp::partial(mars.fit, pred.var = c("perc_alumni"), grid.resolution = 10) %>%
  autoplot()

p2 <- pdp::partial(mars.fit, pred.var =
                     c("perc_alumni", "apps"), 
                   grid.resolution = 10) %>%
  pdp::plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, 
                   screen = list(z = 20, x = -60))

gridExtra::grid.arrange(p1, p2, ncol = 2)

# test error
mars.pred <- predict(mars.fit, newdata = x2)
mars.test.error <- mean((mars.pred - y2)^2)
mars.test.error
```

The regression function is as follow:
$$ outstate = 9781.69 - 0.706 \cdot h(expend - 15687) - 1310.07 \cdot h(grad\_rate - 83) - 1.258 \cdot h(room\_board - 4310) $$
$$- 117.44 \cdot h(perc\_alumni - 21) - 0.395 \cdot h(f\_undergrad - 1411) - 1.475 \cdot h(1411 - f\_undergrad) $$
$$ - 0.654 \cdot h(apps - 7033) + 0.943 \cdot h(personal - 1250) + 4.914 \cdot h(enroll - 910) + 43.21 \cdot h(terminal - 75) $$
$$ + 1234.14 \cdot h(grad\_rate - 82) + 0.679 \cdot h(expend - 6875) - 1.708 \cdot h(2279 - accept) + 0.877 \cdot h(apps - 3624) $$

The partial dependence plot shows a linear increasing of `outstate` as `perc_alumni` increases with a cut point at about 23 % of alumni who donate, holding other variables constant. Also, the plot of `perc_alumni` and `apps` is also shown to visualize their impact on the outcome `outstate`. 

The test error is `r mars.test.error`.

## (c)  GAM

```{r}
# use all the predictors
set.seed(80)
gam.fit <- train(x, y,
                 method = "gam",
                 tuneGrid = data.frame(method = "GCV.Cp", select = TRUE),
                 trControl = ctrl1)

gam.fit$bestTune

gam.fit$finalModel

# Plot the results
par(mar = c(1, 1, 1, 1), mfrow=c(4,4))
plot(gam.fit$finalModel)

# test error
gam.pred <- predict(gam.fit, newdata = x2)
gam.test.error <- mean((gam.pred - y2)^2)
gam.test.error
```

The GAM model includes all the predictors. There is no bivariate function in the model, which means no interaction between predictors.The plot shows the 16 predictors (from left to the right, from top to the bottom) perc_alumni, terminal, books, ph_d, top10perc, grad_rate, top25perc, s_f_ratio, personal, p_undergrad, enroll, room_board, accept, f_undergrad, apps, expend. Among these predictors, it seems like there are 8 nonlinear terms: `room_board`, `grad_rate`,`top25perc`,  `p_undergrad`,`enroll`, `f_undergrad`, `apps`, `expend`.  

The test error is `r gam.test.error`.

## (d) MARS model & linear model comparison

```{r}
# Fit the linear model
set.seed(80)
lm.fit <- train(x, y, method = "lm", trControl = ctrl1)

# Test error
lm.pred <- predict(lm.fit, newdata = x2)
lm.test.error <- mean((lm.pred - y2)^2)
lm.test.error

# resamples
resamp <- resamples(list(lm = lm.fit,
                         mars = mars.fit))
summary(resamp)

# RMSE box-plot between models
bwplot(resamp, metric = "RMSE")
```

Since MARS model has a obvious lower mean RMSE, we prefer to use MARS model over a linear model when predicting the out-of-state tuition in this data example. 

Usually, MARS can outperform linear models, especially when the relationships between predictor variables and the response variable are non-linear. MARS does this by creating piecewise linear models, which can be more flexible than a linear model.

However, whether MARS is a better approach compared to a linear model depends on the specific application and the nature of the data. In some cases, the relationships between predictors and the response variable may be linear, in which a linear model would be better.

In summary, the choice of modeling technique should be based on the nature of the data and the specific research questions. It may also be useful to compare the performance of different modeling techniques using cross-validation to determine which approach is the best for a specific application.



